{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sma import utils, run_snscrape\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the terms and date range for which we wish to pull tweets\n",
    "\n",
    "'''\n",
    "after significant testing i found that pulling large volumes of tweets in a single go can lead to excessive run times.\n",
    "this appears to result from the fact that the time taken to concert the response object - returned by snscrape - to a \n",
    "dataframe increases enormously as the number of tweets returned increases. To avoid this, i suggest slicing your date\n",
    "range to smaller increments and iteratively pulling tweets for each\n",
    "'''\n",
    "#terms = ['natwest', 'barclays', 'santander', 'lloyds banking group']\n",
    "#timeframe = ['2021-01-01', '2021-12-31']\n",
    "\n",
    "term = ['natwest']\n",
    "timeframes = [\n",
    "    ['2021-01-01', '2021-02-28'],\n",
    "    ['2021-03-01', '2021-04-30'],\n",
    "    ['2021-05-01', '2021-06-30'],\n",
    "    ['2021-07-01', '2021-08-31'],\n",
    "    ['2021-09-01', '2021-10-31'],\n",
    "    ['2021-11-01', '2021-12-31']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull tweets for each timeframe in term\n",
    "for timeframe in tqdm(timeframes):\n",
    "    return_df = run_snscrape.scrape_twitter(term, date_range=timeframe, lang='en', limit=None)\n",
    "    \n",
    "    # save the output\n",
    "    output_path = 'data/raw_{}_{}.pickle'.format(term, timeframe[0])\n",
    "    return_df.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate files to create complete dataset\n",
    "consolidated_df = pd.DataFrame()\n",
    "for file in listdir('data'):\n",
    "    if 'raw_{}'.format(term) in file:\n",
    "        temp = pd.read_pickle('data/{}'.format(file))\n",
    "        consolidated_df = pd.concat([consolidated_df, temp], axis=0)\n",
    "\n",
    "#remove duplicate tweets\n",
    "consolidated_df = consolidated_df.drop_duplicates(subset='content', ignore_index=True)\n",
    "\n",
    "print(f'total tweet count: {len(consolidated_df)}')\n",
    "\n",
    "# save a file format that we can view to manually explore the data\n",
    "consolidated_df.to_csv(f'data/master_{term}.csv')\n",
    "# save a version in a format that enables more efficient storage and loading\n",
    "consolidated_df.to_pickle(f'data/master_{term}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4acb7f147a081ceb8d9470e5f1b0c367a56039c7a8cd7008ffc43bc0dc23e952"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
