{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sma import utils, run_snscrape, sentiment\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the terms and date range for which we wish to pull tweets\n",
    "\n",
    "'''\n",
    "after significant testing i found that pulling large volumes of tweets in a single go can lead to excessive run times.\n",
    "this appears to result from the fact that the time taken to concert the response object - returned by snscrape - to a \n",
    "dataframe increases enormously as the number of tweets returned increases. To avoid this, i suggest slicing your date\n",
    "range to smaller increments and iteratively pulling tweets for each\n",
    "'''\n",
    "#terms = ['natwest', 'barclays', 'santander', 'lloyds banking group']\n",
    "#timeframe = ['2021-01-01', '2021-12-31']\n",
    "\n",
    "term = 'natwest'\n",
    "timeframes = [\n",
    "    ['2021-01-01', '2021-02-28'],\n",
    "    ['2021-03-01', '2021-04-30'],\n",
    "    ['2021-05-01', '2021-06-30'],\n",
    "    ['2021-07-01', '2021-08-31'],\n",
    "    ['2021-09-01', '2021-10-31'],\n",
    "    ['2021-11-01', '2021-12-31']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull tweets for each timeframe in term\n",
    "for timeframe in tqdm(timeframes):\n",
    "    return_df = run_snscrape.scrape_twitter(term, date_range=timeframe, lang='en', limit=None)\n",
    "    \n",
    "    # save the output\n",
    "    output_path = 'data/raw_{}_{}.pickle'.format(term, timeframe[0])\n",
    "    return_df.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate files to create complete dataset\n",
    "consolidated_df = pd.DataFrame()\n",
    "for file in listdir('data'):\n",
    "    if 'raw_{}'.format(term) in file:\n",
    "        temp = pd.read_pickle('data/{}'.format(file))\n",
    "        consolidated_df = pd.concat([consolidated_df, temp], axis=0)\n",
    "\n",
    "#remove duplicate tweets\n",
    "consolidated_df = consolidated_df.drop_duplicates(subset='content', ignore_index=True)\n",
    "\n",
    "print(f'total tweet count: {len(consolidated_df)}')\n",
    "\n",
    "# save a file format that we can view to manually explore the data\n",
    "consolidated_df.to_csv(f'data/master_{term}.csv')\n",
    "# save a version in a format that enables more efficient storage and loading\n",
    "consolidated_df.to_pickle(f'data/master_{term}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76145/76145 [05:55<00:00, 214.01it/s]\n"
     ]
    }
   ],
   "source": [
    "master_df = pd.read_pickle(f'data/master_{term}.pickle')\n",
    "\n",
    "# we will use Vader for sentiment, and return the full polarity output\n",
    "tqdm.pandas()\n",
    "new_cols = ['sent_positive', 'sent_negative', 'sent_neutral', 'sent_compound', 'sentiment']\n",
    "master_df[new_cols] = master_df.progress_apply(lambda x: sentiment.vader_sentiment(x['content'], return_polarities=True), axis=1, result_type='expand')\n",
    "\n",
    "# save file\n",
    "master_df.to_pickle(f'data/sentiment_{term}.pickle')\n",
    "master_df.to_csv(f'data/sentiment_{term}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4acb7f147a081ceb8d9470e5f1b0c367a56039c7a8cd7008ffc43bc0dc23e952"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
